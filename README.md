# ReproduÃ§Ã£o: Img-Diff (CVPR 2025) - MÃ¡rio Guerra

Este repositÃ³rio contÃ©m a implementaÃ§Ã£o e reproduÃ§Ã£o dos mÃ©todos propostos no artigo **"Img-Diff: Contrastive Data Synthesis for Multimodal Large Language Models"**, desenvolvido como projeto final da disciplina de VisÃ£o Computacional. O projeto implementa um pipeline automatizado para a sÃ­ntese de dados contrastivos, visando melhorar a capacidade de MLLMs (Multimodal LLMs) em detectar diferenÃ§as finas e granulares entre imagens semelhantes.

## ğŸ“‹ Estrutura do Projeto

O pipeline foi implementado seguindo a metodologia do artigo, dividido em trÃªs mÃ³dulos principais:

1. **GeraÃ§Ã£o de Pares (Module A):** GeraÃ§Ã£o de pares de imagens quase idÃªnticos focados em substituiÃ§Ã£o de objetos (*Object Replacement*), utilizando *Stable Diffusion XL*.
2. **DetecÃ§Ã£o de DiferenÃ§as (Module B - Difference Area Generator):** IdentificaÃ§Ã£o e segmentaÃ§Ã£o das regiÃµes modificadas utilizando *FastSAM* para segmentaÃ§Ã£o e *CLIP* para verificaÃ§Ã£o de similaridade semÃ¢ntica.
3. **GeraÃ§Ã£o de Legendas (Module C - Difference Captions Generator):** DescriÃ§Ã£o textual das diferenÃ§as utilizando um MLLM (*LLaVA*) para criar o dataset final no formato de instruÃ§Ã£o visual.

## OrganizaÃ§Ã£o de Pastas
```text
img-diff-repro/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ generated_images/   # SaÃ­da do MÃ³dulo A (Pares de Imagens)
â”‚   â””â”€â”€ final_dataset/      # SaÃ­da do MÃ³dulo C (JSON Final)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ generation/         # Scripts de geraÃ§Ã£o (SDXL + Prompt Engineering)
â”‚   â”œâ”€â”€ processing/         # Scripts de detecÃ§Ã£o (FastSAM + CLIP + IoU Filter)
â”‚   â””â”€â”€ captioning/         # Scripts de descriÃ§Ã£o (LLaVA-NeXT/1.5)
â””â”€â”€ requirements.txt        # DependÃªncias do projeto
```

## ğŸš€ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

**Importante:** Devido a conflitos de versÃ£o conhecidos entre torchvision, fastsam e bibliotecas de aceleraÃ§Ã£o, siga estritamente os passos abaixo para configurar o ambiente.

### 1. Criar Ambiente Conda

```bash
conda create -n img-diff python=3.10 -y
conda activate img-diff
```

### 2. Instalar PyTorch (Manual)

Ã‰ crucial instalar o PyTorch compatÃ­vel com CUDA antes das outras bibliotecas para evitar conflitos de dependÃªncia.

```bash
# Para Linux/Windows com GPU NVIDIA (CUDA 12.1)
pip install torch torchvision --index-url [https://download.pytorch.org/whl/cu121](https://download.pytorch.org/whl/cu121)
```

### 3. Instalar DependÃªncias Gerais

```bash
pip install -r requirements.txt
```

### 4. Instalar Ferramentas EspecÃ­ficas

Algumas ferramentas essenciais do artigo precisam ser instaladas diretamente dos repositÃ³rios oficiais.

```bash
# CLIP da OpenAI (Para cÃ¡lculo de similaridade de cosseno)
pip install git+[https://github.com/openai/CLIP.git](https://github.com/openai/CLIP.git)

# Ultralytics (NecessÃ¡rio para rodar o FastSAM)
pip install ultralytics

# BitsAndBytes (Opcional: Para rodar LLaVA em 4-bit se tiver pouca VRAM)
pip install bitsandbytes
```

## ğŸ› ï¸ Como Executar

Siga a ordem do pipeline para gerar o dataset completo.

### Passo 1: Gerar Pares de Imagens (Generation)

Este script utiliza o SDXL com sementes fixas para gerar pares de imagens (Original vs Modificada) baseadas em prompts de substituiÃ§Ã£o.

```bash
python /scripts/01_generate_pairs.py
```

**SaÃ­da:** As imagens serÃ£o salvas em ```data/generated_images/``` (pastas ```left``` e ```right```).

### Passo 2: Detectar Ãreas de DiferenÃ§a (Detection)

Analisa os pares gerados, aplica o FastSAM para segmentar objetos e utiliza o CLIP para validar quais regiÃµes sofreram alteraÃ§Ãµes visuais significativas (< 0.85 similaridade).

```bash
python /scripts/02_filter_differences.py
```

**SaÃ­da 1:** O script gera imagens com bounding boxes vermelhas para visualizaÃ§Ã£o das diferenÃ§as detectadas que sÃ£o salvas em ```data/caption_images/``` (pastas ```left``` e ```right```).

**SaÃ­da 2:** O script gera arquivos json com as coordenadas das bounding boxes, a similaridade e os paths para cada par de imagens (sÃ£o salvos em ```data/raw_captions/```).

### Passo 3: Gerar Legendas e JSON Final (Captioning)

Utiliza o LLaVA para gerar descriÃ§Ãµes semÃ¢nticas das diferenÃ§as encontradas e consolida tudo em um arquivo JSON pronto para treinamento.

```bash
python /scripts/03_create_json.py
```

**SaÃ­da Final:** ```data/final_dataset/img_diff_train.json```

## ğŸ“Š Exemplo de Resultado (JSON)

O arquivo final segue o formato de Visual Instruction Tuning descrito no paper:

```json
{
  "image_left": "path/to/img_left.jpg",
  "image_right": "path/to/img_right.jpg",
  "bbox": [0.45, 0.20, 0.65, 0.50],
  "conversations": [
    {
      "from": "human",
      "value": "Analyze the left image and the right image. What is the difference between the red bounding box area in each image?"
    },
    {
      "from": "gpt",
      "value": "The left image shows a woman holding a black camera, while
                the right image shows the same woman holding a stack of white plates. The difference is
                that the woman is now holding plates instead of a camera."
    }
  ],
  "captions1": "A black camera.",
  "captions2": "A stack of white plates."
}
```

## ğŸ“š ReferÃªncias

**Artigo Original:** Jiao, Q., Chen, D., Huang, Y., Ding, B., Li, Y., & Shen, Y. (2024). Img-Diff: Contrastive Data Synthesis for Multimodal Large Language Models. arXiv preprint arXiv:2408.04594. 

**FastSAM:** Zhao, X., et al. (2023). Fast Segment Anything. 

**SDXL:** Podell, D., et al. (2023). SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis.