# ReproduÃ§Ã£o: Img-Diff (CVPR 2025) - MÃ¡rio Guerra

Este repositÃ³rio contÃ©m a implementaÃ§Ã£o e reproduÃ§Ã£o dos mÃ©todos propostos no artigo **"Img-Diff: Contrastive Data Synthesis for Multimodal Large Language Models"**, desenvolvido como projeto final da disciplina de VisÃ£o Computacional. O projeto implementa um pipeline automatizado para a sÃ­ntese de dados contrastivos, visando melhorar a capacidade de MLLMs (Multimodal LLMs) em detectar diferenÃ§as finas e granulares entre imagens semelhantes.

## ğŸ“‹ Estrutura do Projeto

O pipeline foi implementado seguindo a metodologia do artigo, dividido em trÃªs mÃ³dulos principais:

1. **GeraÃ§Ã£o de Pares (Module A):** GeraÃ§Ã£o de pares de imagens quase idÃªnticos focados em substituiÃ§Ã£o de objetos (*Object Replacement*), utilizando *Stable Diffusion XL*.
2. **DetecÃ§Ã£o de DiferenÃ§as (Module B - Difference Area Generator):** IdentificaÃ§Ã£o e segmentaÃ§Ã£o das regiÃµes modificadas utilizando *FastSAM* para segmentaÃ§Ã£o e *CLIP* para verificaÃ§Ã£o de similaridade semÃ¢ntica.
3. **GeraÃ§Ã£o de Legendas (Module C - Difference Captions Generator):** DescriÃ§Ã£o textual das diferenÃ§as utilizando um MLLM (*LLaVA*) para criar o dataset final no formato de instruÃ§Ã£o visual.

## OrganizaÃ§Ã£o de Pastas
```text
img-diff-repro/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ generated_images/   # SaÃ­da do MÃ³dulo A (Pares de Imagens)
â”‚   â”œâ”€â”€ caption_images/     # SaÃ­da 1 do MÃ³dulo B (Pares de Imagens com bounding box)
â”‚   â”œâ”€â”€ raw_captions/       # SaÃ­da 2 do MÃ³dulo B (json com coordenadas e similaridade das imagens)
â”‚   â””â”€â”€ final_dataset/      # SaÃ­da do MÃ³dulo C (JSON Final para treinamento de VLM)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 01_generate_pairs.py      # Scripts de geraÃ§Ã£o (SDXL + Prompt Engineering)
â”‚   â”œâ”€â”€ 02_filter_differences.py  # Scripts de detecÃ§Ã£o (FastSAM + CLIP + IoU Filter)
â”‚   â””â”€â”€ 03_create_json.py         # Scripts de descriÃ§Ã£o (LLaVA-NeXT/1.5)
â”œâ”€â”€ README.md               # Este arquivo
â””â”€â”€ requirements.txt        # DependÃªncias do projeto
```

## ğŸš€ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

**Importante:** Devido a conflitos de versÃ£o conhecidos entre torchvision, fastsam e bibliotecas de aceleraÃ§Ã£o, siga estritamente os passos abaixo para configurar o ambiente.

### 1. Criar Ambiente Conda

```bash
conda create -n img-diff python=3.10 -y
conda activate img-diff
```

### 2. Instalar PyTorch (Manual)

Ã‰ crucial instalar o PyTorch compatÃ­vel com CUDA antes das outras bibliotecas para evitar conflitos de dependÃªncia.

```bash
# Para Linux/Windows com GPU NVIDIA (CUDA 12.1)
pip install torch torchvision --index-url [https://download.pytorch.org/whl/cu121](https://download.pytorch.org/whl/cu121)
```

### 3. Instalar DependÃªncias Gerais

```bash
pip install -r requirements.txt
```

### 4. Instalar Ferramentas EspecÃ­ficas

Algumas ferramentas essenciais do artigo precisam ser instaladas diretamente dos repositÃ³rios oficiais.

```bash
# CLIP da OpenAI (Para cÃ¡lculo de similaridade de cosseno)
pip install git+[https://github.com/openai/CLIP.git](https://github.com/openai/CLIP.git)

# Ultralytics (NecessÃ¡rio para rodar o FastSAM)
pip install ultralytics

# BitsAndBytes (Opcional: Para rodar LLaVA em 4-bit se tiver pouca VRAM)
pip install bitsandbytes
```

## ğŸ› ï¸ Como Executar

Siga a ordem do pipeline para gerar o dataset completo.

### Passo 1: Gerar Pares de Imagens (Generation)

Este script utiliza o SDXL com sementes fixas para gerar pares de imagens (Original vs Modificada) baseadas em prompts de substituiÃ§Ã£o.

```bash
python /scripts/01_generate_pairs.py
```

**SaÃ­da:** As imagens serÃ£o salvas em ```data/generated_images/``` (pastas ```left``` e ```right```).

### Passo 2: Detectar Ãreas de DiferenÃ§a (Detection)

Analisa os pares gerados, aplica o FastSAM para segmentar objetos e utiliza o CLIP para validar quais regiÃµes sofreram alteraÃ§Ãµes visuais significativas (< 0.85 similaridade).

```bash
python /scripts/02_filter_differences.py
```

**SaÃ­da 1:** O script gera imagens com bounding boxes vermelhas para visualizaÃ§Ã£o das diferenÃ§as detectadas que sÃ£o salvas em ```data/caption_images/``` (pastas ```left``` e ```right```).

**SaÃ­da 2:** O script gera arquivos json com as coordenadas das bounding boxes, a similaridade e os paths para cada par de imagens (sÃ£o salvos em ```data/raw_captions/```).

### Passo 3: Gerar Legendas e JSON Final (Captioning)

Utiliza o LLaVA para gerar descriÃ§Ãµes semÃ¢nticas das diferenÃ§as encontradas e consolida tudo em um arquivo JSON pronto para treinamento.

```bash
python /scripts/03_create_json.py
```

**SaÃ­da Final:** ```data/final_dataset/img_diff_train.json```

## ğŸ“Š Exemplo de Resultado (JSON)

O arquivo final segue o formato de Visual Instruction Tuning descrito no paper:

<img src="data/exemplo_saida.jpg" width="600px" />


```json
{
  "conversations": [
    {
      "from": "human",
      "value": "Analyse the left image and the right image (separated by the black vertical bar). What is the difference between the red bounding box area in each image? Answer the question in a few concise sentences"
    },
    {
      "from": "llm",
      "value": "The difference between the red bounding box area in the left and right images is that in the left image, the cat is inside the red box, while in the right image, the dog is inside the red box."
    }
  ],
  "bbox": [
    0.29,
    0.29,
    0.92,
    0.77
  ],
  "captions1": "The main object in this image is a cat.",
  "captions2": "The main object in this image is a dog.",
  "image_left": "data/caption_images/left/caption_0.jpg",
  "image_right": "data/caption_images/right/caption_0.jpg"
}
```

## ğŸ“š ReferÃªncias

**Artigo Original:** Jiao, Q., Chen, D., Huang, Y., Ding, B., Li, Y., & Shen, Y. (2024). Img-Diff: Contrastive Data Synthesis for Multimodal Large Language Models. arXiv preprint arXiv:2408.04594. 

**FastSAM:** Zhao, X., et al. (2023). Fast Segment Anything. 

**SDXL:** Podell, D., et al. (2023). SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis.